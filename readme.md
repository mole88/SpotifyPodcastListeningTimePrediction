# Podcast Listening Time — Regression with a Fully-Connected Neural Network

**Кратко:** проект предсказывает время прослушивания эпизода подкаста в минутах по метаданным эпизода и информации о рекламных блоках. Модель — полносвязная нейронная сеть (PyTorch) с Batch Normalization и Dropout для более стабильного обучения и лучшей обобщающей способности.

---

## Содержание

- [Описание](#описание)
- [Датасет](#датасет)
- [Предобработка данных](#предобработка-данных)
- [Архитектура модели](#архитектура-модели)
- [Обучение и оценка](#обучение-и-оценка)
- [Результаты](#результаты)

---

## Описание

Цель проекта — построить регрессионную модель, которая по характеристикам эпизода (названию подкаста, жанру, длительности, популярности гостя, количестве рекламных блоков и др.) предсказывает `Listening_Time_minutes`. В работе выполнены EDA, базовая предобработка, фиче‑инжиниринг, обучение MLP в PyTorch и подготовка сабмита для Kaggle.

## Датасет

Данные взяты из соревнования на Kaggle: `playground-series-s5e4` (файлы `train.csv` и `test.csv`).

**Целевая переменная:** `Listening_Time_minutes`.

(Если есть возможность — добавьте сюда краткую информацию о размере выборки и числе исходных признаков.)

## Предобработка данных

Ключевые шаги предобработки, реализованные в ноутбуке:

- Удаление выбросов: удалены записи с `Listening_Time_minutes < 1`.
- Заполнение пропусков:
  - `Episode_Length_minutes` — медиана;
  - `Guest_Popularity_percentage` — медиана;
  - `Number_of_Ads` — мода; значения > 4 заменены на моду.
- Фиче-инжиниринг:
  - извлечение номера эпизода из `Episode_Title` (если есть число) и удаление исходной текстовой колонки;
- Кодирование категорий: `pd.get_dummies` для `Podcast_Name`, `Genre`, `Publication_Day`, `Publication_Time`, `Episode_Sentiment`;
- Масштабирование числовых признаков: `StandardScaler`.

В ноутбуке также присутствуют визуализации: распределение целевой переменной и тепловая карта корреляций.

## Архитектура модели

Использована последовательная MLP в PyTorch (вход — 78 признаков):

- `Linear(78 -> 128)`
- `ReLU`
- `BatchNorm1d(128)`
- `Dropout(0.3)`
- `Linear(128 -> 64)`
- `ReLU`
- `BatchNorm1d(64)`
- `Dropout(0.3)`
- `Linear(64 -> 32)`
- `ReLU`
- `Linear(32 -> 1)` — выход (предсказание в минутах)

**Гиперпараметры:** Adam (lr=1e-3), MSELoss, batch size = 512, epochs = 30. Обучение выполнялось на GPU, если оно доступно.

## Обучение и оценка

Реализованы циклы `train` и `val`, которые вычисляют средний MSE по батчам и логируют потери для каждой эпохи. Для мониторинга построены графики потерь (train vs validation). Финальные предсказания на `test` сохраняются в `subm.csv` для отправки на Kaggle.

**Метрика соревнования:** RMSE (root mean squared error) — note: в коде оптимизируется MSE, оценка на публичном лидборде — RMSE.

## Результаты

- Пример публичного результата: **RMSE = 13.5**
- Результат лидера (public): **RMSE = 11.5**

---

Если хотите, могу дополнительно:

- добавить раздел с краткими командами запуска (Colab / локально),
- вынести код в модульную структуру (`src/`) и подготовить `requirements.txt`,
- перевести README на английский.

Скажите, что предпочитаете — внесу изменения.

